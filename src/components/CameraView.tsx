import React, { useEffect, useRef, useState } from "react";
import {
  FilesetResolver,
  HandLandmarker,
  FaceLandmarker,
} from "@mediapipe/tasks-vision";
import "./CameraView.css";

type Overlay = {
  id: number;
  src: string;
  name: string;
  x: number;
  y: number;
  scale: number;
  visible: boolean;
};

const CameraView: React.FC = () => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const captureCanvasRef = useRef<HTMLCanvasElement>(null);
  const handLandmarkerRef = useRef<HandLandmarker | null>(null);
  const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
  const animationFrameRef = useRef<number>();

  const [overlay, setOverlay] = useState<Overlay>({
    id: 1,
    name: "ã‚“ã½ãŸã",
    src: "/npo.png",
    x: window.innerWidth / 2,
    y: window.innerHeight / 2,
    scale: 1.0,
    visible: false,
  });

  const [previewImage, setPreviewImage] = useState<string | null>(null);

  // === ã‚«ãƒ¡ãƒ©èµ·å‹• ===
  useEffect(() => {
    const startCamera = async () => {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: "user",
          width: { ideal: window.innerWidth },
          height: { ideal: window.innerHeight },
        },
      });
      if (videoRef.current) videoRef.current.srcObject = stream;
    };
    startCamera();

    return () => {
      const tracks = (videoRef.current?.srcObject as MediaStream)?.getTracks();
      tracks?.forEach((t) => t.stop());
    };
  }, []);

  // === MediaPipe åˆæœŸåŒ– ===
  useEffect(() => {
    const initModels = async () => {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
      );

      const [handLandmarker, faceLandmarker] = await Promise.all([
        HandLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath:
              "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
          },
          runningMode: "VIDEO",
          numHands: 1,
        }),
        FaceLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath:
              "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
          },
          runningMode: "VIDEO",
          numFaces: 1,
        }),
      ]);

      handLandmarkerRef.current = handLandmarker;
      faceLandmarkerRef.current = faceLandmarker;

      if (videoRef.current) videoRef.current.onloadeddata = () => detectLoop();
    };

    initModels();

    const detectLoop = async () => {
      const video = videoRef.current;
      if (!video || !handLandmarkerRef.current || !faceLandmarkerRef.current) {
        requestAnimationFrame(detectLoop);
        return;
      }
      if (video.videoWidth === 0) {
        requestAnimationFrame(detectLoop);
        return;
      }

      const now = Date.now();
      const handResult = await handLandmarkerRef.current.detectForVideo(video, now);
      const faceResult = await faceLandmarkerRef.current.detectForVideo(video, now);

      let isPalm = false;
      let newX = overlay.x;
      let newY = overlay.y;
      let newScale = overlay.scale;

      // === æ‰‹ã®ã²ã‚‰åˆ¤å®š ===
      if (handResult.landmarks?.[0]) {
        isPalm = detectPalmFacingCamera(handResult.landmarks[0]);
      }

      // === é¡”ä½ç½®ã«åŸºã¥ã„ã¦ã‚“ã½ãŸãé…ç½® ===
      if (faceResult.faceLandmarks?.[0]) {
        const face = faceResult.faceLandmarks[0];
        const nose = face[1];
        const left = face[234];
        const right = face[454];
      
        if (nose && left && right && videoRef.current) {
          const video = videoRef.current;
          const videoW = video.videoWidth;
          const videoH = video.videoHeight;
      
          // ==== ãƒ“ãƒ‡ã‚ªã®å®Ÿã‚µã‚¤ã‚º â†’ ç”»é¢ä¸Šã®è¡¨ç¤ºç¯„å›² ====
          const rect = video.getBoundingClientRect();
      
          // object-fit: cover ã®ã‚ºãƒ¬è£œæ­£
          const videoAspect = videoW / videoH;
          const viewAspect = rect.width / rect.height;
          let drawX = rect.x;
          let drawY = rect.y;
          let scaleX = rect.width / videoW;
          let scaleY = rect.height / videoH;
      
          if (videoAspect > viewAspect) {
            // æ¨ªãŒåºƒã„ï¼ˆå·¦å³ãƒˆãƒªãƒŸãƒ³ã‚°ï¼‰
            const scaledVideoW = videoH * viewAspect;
            const offsetX = (videoW - scaledVideoW) / 2;
            scaleX = rect.width / scaledVideoW;
            drawX = rect.x - offsetX * scaleX;
          } else if (videoAspect < viewAspect) {
            // ç¸¦ãŒåºƒã„ï¼ˆä¸Šä¸‹ãƒˆãƒªãƒŸãƒ³ã‚°ï¼‰
            const scaledVideoH = videoW / viewAspect;
            const offsetY = (videoH - scaledVideoH) / 2;
            scaleY = rect.height / scaledVideoH;
            drawY = rect.y - offsetY * scaleY;
          }
      
          // ==== é¡”åº§æ¨™ã‚’ç”»é¢åº§æ¨™ã¸å¤‰æ› ====
          const toScreen = (p: any) => ({
            x: drawX + p.x * videoW * scaleX,
            y: drawY + p.y * videoH * scaleY,
          });
      
          const nosePos = toScreen(nose);
          const leftPos = toScreen(left);
          const rightPos = toScreen(right);
      
          const faceWidthPx = Math.abs(rightPos.x - leftPos.x);
          const placeRight = nosePos.x < window.innerWidth / 2;
          const offsetX = placeRight ? faceWidthPx * 1.3: -faceWidthPx * 1.3;
      
          newX = nosePos.x + offsetX;
          newY = nosePos.y - faceWidthPx; // å°‘ã—ä¸Šã«
          newScale = Math.min(Math.max(faceWidthPx / 150, 0.8), 2.0);
      
          // ç”»é¢å¤–è£œæ­£
          newX = Math.min(
            Math.max(newX, (150 * newScale) / 2),
            window.innerWidth - (150 * newScale) / 2
          );
          newY = Math.min(
            Math.max(newY, (150 * newScale) / 2),
            window.innerHeight - (150 * newScale) / 2
          );
        }
      }
      

      // ã‚¹ãƒ ãƒ¼ã‚ºã«ç§»å‹•
      setOverlay((prev) => ({
        ...prev,
        visible: isPalm,
        x: lerp(prev.x, newX, 0.25),
        y: lerp(prev.y, newY, 0.25),
        scale: lerp(prev.scale, newScale, 0.2),
      }));

      animationFrameRef.current = requestAnimationFrame(detectLoop);
    };

    return () => cancelAnimationFrame(animationFrameRef.current!);
  }, []);

  // === æ‰‹ã®ã²ã‚‰åˆ¤å®š ===
  const detectPalmFacingCamera = (hand: any[]): boolean => {
    const zValues = hand.map((p) => p.z);
    const zRange = Math.max(...zValues) - Math.min(...zValues);
    const avgZ = zValues.reduce((a, b) => a + b, 0) / zValues.length;
    return avgZ < 0 && zRange < 0.15;
  };

  const lerp = (a: number, b: number, t: number) => a + (b - a) * t;

  // === æ’®å½±å‡¦ç† ===
  const handleCapture = () => {
    if (!videoRef.current || !captureCanvasRef.current) return;

    const video = videoRef.current;
    const canvas = captureCanvasRef.current;
    const ctx = canvas.getContext("2d");
    if (!ctx) return;

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // ã‚«ãƒ¡ãƒ©æ˜ åƒã‚’æç”»
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    // ã‚“ã½ãŸãã‚’æç”»
    if (overlay.visible) {
      const img = new Image();
      img.src = overlay.src;
      img.onload = () => {
        const scale = 150 * overlay.scale;
        const drawX = (overlay.x / window.innerWidth) * canvas.width;
        const drawY = (overlay.y / window.innerHeight) * canvas.height;
        ctx.drawImage(img, drawX - scale / 2, drawY - scale / 2, scale, scale);
        setPreviewImage(canvas.toDataURL("image/png"));
      };
    } else {
      setPreviewImage(canvas.toDataURL("image/png"));
    }
  };

  // === ä¿å­˜ ===
  const handleSave = () => {
    if (!previewImage) return;
    const a = document.createElement("a");
    a.href = previewImage;
    a.download = `npocamera_${Date.now()}.png`;
    a.click();
  };

  // === å…±æœ‰ ===
  const handleShare = async () => {
    if (!previewImage) return;
    try {
      const res = await fetch(previewImage);
      const blob = await res.blob();
      const file = new File([blob], "npo.png", { type: "image/png" });
      if (navigator.canShare && navigator.canShare({ files: [file] })) {
        await navigator.share({
          title: "ã‚“ã½ãŸãã‚«ãƒ¡ãƒ©ğŸ“¸",
          text: "ã‚“ã½ãŸãã‚«ãƒ¡ãƒ©ã§æ’®ã£ãŸã‚ˆï¼",
          files: [file],
        });
      } else {
        const tweetUrl = `https://twitter.com/intent/tweet?text=${encodeURIComponent(
          "#ã‚“ã½ãŸãã‚«ãƒ¡ãƒ©ã§æ’®ã£ãŸã‚ˆ"
        )}`;
        window.open(tweetUrl, "_blank");
      }
    } catch (err) {
      console.error("å…±æœ‰ã«å¤±æ•—:", err);
    }
  };

  return (
    <div className="camera-container">
      <video ref={videoRef} autoPlay playsInline muted className="camera-video" />
      <canvas ref={captureCanvasRef} style={{ display: "none" }} />

      {/* === ã‚“ã½ãŸã === */}
      {overlay.visible && (
        <img
          src={overlay.src}
          alt={overlay.name}
          className="overlay-image"
          style={{
            top: `${overlay.y}px`,
            left: `${overlay.x}px`,
            width: `${150 * overlay.scale}px`,
            transform: "translate(-50%, -50%)",
          }}
        />
      )}

      {/* === æ’®å½±ãƒœã‚¿ãƒ³ === */}
      {!previewImage && (
        <div className="camera-ui">
          <button className="capture-btn" onClick={handleCapture}></button>
        </div>
      )}

      {/* === ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆãƒã‚§ã‚­é¢¨ï¼‰ === */}
      {previewImage && (
        <div className="preview-overlay">
          <div className="preview-frame">
            <img src={previewImage} alt="preview" />
            <div className="preview-buttons">
              <button className="save-btn" onClick={handleSave}>ä¿å­˜</button>
              <button className="x-btn" onClick={handleShare}>ğ•ã«ãƒã‚¹ãƒˆ</button>
              <button className="close-btn" onClick={() => setPreviewImage(null)}>æˆ»ã‚‹</button>
            </div>
          </div>
        </div>
      )}
    </div>
  );
};

export default CameraView;
